{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2622a64",
   "metadata": {},
   "source": [
    "### Step 1: Environment Setup\n",
    "\n",
    "**Description:** Install the necessary libraries for fine-tuning the Ministral 3 model, including Hugging Face `transformers`, `trl` for supervised fine-tuning, and `bitsandbytes` for memory optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08db5480",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install -U transformers trl datasets accelerate bitsandbytes flash-attn mistral_common"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af329a6a",
   "metadata": {},
   "source": [
    "### Step 2: Hardware and CUDA Verification\n",
    "\n",
    "**Description:** Check the environment to ensure the GPU is recognized correctly and verify the available VRAM to determine if the setup can handle the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c60fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device_props = torch.cuda.get_device_properties(0)\n",
    "    print(f\"GPU: {device_props.name}\")\n",
    "    print(f\"Total VRAM: {device_props.total_memory / 1e9:.2f} GB\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a6a92a5",
   "metadata": {},
   "source": [
    "### Step 3: Library Imports\n",
    "\n",
    "**Description:** Load core modules for model architecture, dataset management, and the SFT (Supervised Fine-Tuning) trainer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f44cce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    Mistral3ForConditionalGeneration, \n",
    "    MistralCommonBackend\n",
    ")\n",
    "from trl import SFTTrainer, SFTConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b171d6d",
   "metadata": {},
   "source": [
    "### Step 4: Model and Tokenizer Initialization\n",
    "\n",
    "**Description:** Load the model with `bfloat16` precision and configure the chat template. This step also includes freezing the vision-related parameters (vision tower and projector) to focus training strictly on the language capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301c1d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"0xA50C1A1/Ministral-3-8B-Instruct-2512-BF16-Heretic\"\n",
    "\n",
    "# Load the model in BF16\n",
    "model = Mistral3ForConditionalGeneration.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# Load and configure tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "\n",
    "# Define custom chat template for Ministral 3\n",
    "tokenizer.chat_template = \"\"\"\n",
    "{%- if messages[0]['role'] == 'system' -%}\n",
    "    {{- bos_token + '[SYSTEM_PROMPT]' + messages[0]['content'] + '[/SYSTEM_PROMPT]' -}}\n",
    "    {%- set loop_messages = messages[1:] -%}\n",
    "{%- else -%}\n",
    "    {{- bos_token -}}\n",
    "    {%- set loop_messages = messages -%}\n",
    "{%- endif -%}\n",
    "\n",
    "{%- for message in loop_messages -%}\n",
    "    {%- if message['role'] == 'user' -%}\n",
    "        {{- '[INST]' + message['content'] + '[/INST]' -}}\n",
    "    {%- elif message['role'] == 'assistant' -%}\n",
    "        {%- generation -%}\n",
    "            {{- message['content'] + eos_token -}}\n",
    "        {%- endgeneration -%}\n",
    "    {%- endif -%}\n",
    "{%- endfor -%}\n",
    "\n",
    "{%- if add_generation_prompt -%}\n",
    "    {%- generation -%}{%- endgeneration -%}\n",
    "{%- endif -%}\n",
    "\"\"\".strip()\n",
    "\n",
    "# Freeze Vision-related components\n",
    "for param in model.model.vision_tower.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in model.model.multi_modal_projector.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Model loaded. Trainable parameters: {trainable_params/1e6:.2f}M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b098fd",
   "metadata": {},
   "source": [
    "### Step 5: Dataset Loading\n",
    "\n",
    "**Description:** Import the training dataset from Hugging Face. We apply a shuffle to ensure better stochastic gradient descent during the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d261bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"0xA50C1A1/rp_dataset\", split=\"train\").shuffle(seed=3407)\n",
    "print(f\"Dataset loaded: {len(dataset)} samples ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc4e891",
   "metadata": {},
   "source": [
    "### Step 6: SFT Trainer Configuration\n",
    "\n",
    "**Description:** Set up the fine-tuning hyper-parameters. We use `SFTConfig` for efficient memory management, including gradient checkpointing and NEFTune noise for better generalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc8a3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "sft_config = SFTConfig(\n",
    "    output_dir=\"outputs\",\n",
    "    dataset_text_field=\"messages\",\n",
    "    packing=True, # See https://github.com/huggingface/trl/pull/3749\n",
    "    padding_free=True,\n",
    "    assistant_only_loss=True,\n",
    "    \n",
    "    # Training Hyperparameters\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=2,\n",
    "    learning_rate=1e-5,\n",
    "    \n",
    "    # Precision and Efficiency\n",
    "    bf16=True,\n",
    "    gradient_checkpointing=True,\n",
    "    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
    "    \n",
    "    # Optimizer & Scheduler\n",
    "    optim=\"adamw_torch_fused\",\n",
    "    weight_decay=0.05,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_steps=0.1,\n",
    "    max_grad_norm=1.0,\n",
    "    max_length=8192,\n",
    "    \n",
    "    # Logging\n",
    "    logging_steps=5,\n",
    "    report_to=\"none\",\n",
    "    \n",
    "    # Regularization\n",
    "    seed=3407,\n",
    "    neftune_noise_alpha=5,\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=sft_config,\n",
    "    train_dataset=dataset,\n",
    "    processing_class=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1942ca",
   "metadata": {},
   "source": [
    "### Step 7: Training Execution\n",
    "\n",
    "**Description:** Initializing the training loop. We perform a manual garbage collection and clear the CUDA cache to prevent Out-Of-Memory (OOM) errors at the start of the run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70c5c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1050f873",
   "metadata": {},
   "source": [
    "### Step 8: Model Inference and Testing\n",
    "\n",
    "**Description:** Run a qualitative test on the fine-tuned model. We switch the model to evaluation mode and generate a response using the updated chat template to verify the learning results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c473ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear memory and set to eval mode\n",
    "torch.cuda.empty_cache()\n",
    "model.eval()\n",
    "\n",
    "# Sample prompt\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"Roleplay as an tavern-keeper. Describe your actions using *asterisks* and wrap your speech in \\\"double quotes\\\".\"},\n",
    "    {\"role\": \"user\", \"content\": \"*I walk into the tavern and ask for an ale.*\"}\n",
    "]\n",
    "\n",
    "# Prepare inputs\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages, \n",
    "    add_generation_prompt=True, \n",
    "    return_tensors=\"pt\",\n",
    "    return_dict=True\n",
    ").to(model.device)\n",
    "\n",
    "# Generate response\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs, \n",
    "        max_new_tokens=300, \n",
    "        do_sample=True, \n",
    "        temperature=1.0,\n",
    "        min_p=0.05,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        pad_token_id=tokenizer.pad_token_id or tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "# Decode output (removing the input tokens)\n",
    "input_length = inputs[\"input_ids\"].shape[1]\n",
    "response = tokenizer.decode(outputs[0][input_length:], skip_special_tokens=True)\n",
    "\n",
    "print(f\"Prompt: {messages[1]['content']}\")\n",
    "print(f\"Response:\\n{response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f33991",
   "metadata": {},
   "source": [
    "### Step 9: Model Export and Upload to Hugging Face Hub\n",
    "\n",
    "**Description:** Authenticate with the Hugging Face Hub and upload the fine-tuned model, tokenizer, and training configuration. This allows for easy sharing, versioning, and deployment of your model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5dd3ea0",
   "metadata": {},
   "source": [
    "##### 9.1: Hub Authentication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40ce546",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "# Put your HF Write Token here\n",
    "# It is recommended to use an environment variable or secret for security\n",
    "HF_TOKEN = \"your_hf_token\" \n",
    "\n",
    "login(token=HF_TOKEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d188f4a6",
   "metadata": {},
   "source": [
    "##### 9.2: Push to Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e05854c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Fetch the official tokenizer to grab its template\n",
    "official_repo = \"mistralai/Ministral-3-8B-Instruct-2512-BF16\"\n",
    "official_tokenizer = AutoTokenizer.from_pretrained(official_repo, trust_remote_code=True)\n",
    "\n",
    "# Synchronize the template\n",
    "tokenizer.chat_template = official_tokenizer.chat_template\n",
    "\n",
    "print(\"Official chat template restored successfully.\")\n",
    "\n",
    "# Set your target repository name\n",
    "hub_model_id = \"YourUsername/Ministral-3-8B-Instruct-RP\"\n",
    "\n",
    "# Push everything to the Hub\n",
    "trainer.push_to_hub(repo_id=hub_model_id)\n",
    "\n",
    "# Explicitly push the tokenizer to ensure the restored template is saved\n",
    "tokenizer.push_to_hub(hub_model_id)\n",
    "\n",
    "print(f\"Model and Tokenizer are now live at: https://huggingface.co/{hub_model_id}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
